{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Demand Forecasting with Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy raw data to S3\n",
    "\n",
    "The dataset we use here is the **multivariate time-series** [electricity](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014) data taken from *Dua, D. and Graff, C. (2019). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml), Irvine, CA: University of California, School of Information and Computer Science.* A cleaned version of the data containing **321** time-series with **1H** frequency, starting from **2012-01-01** with **26304** time-steps, is available to download directly via [gluonts](https://github.com/awslabs/gluon-ts).\n",
    "\n",
    "For the ease of access, with have made the cleaned data available in the following S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "original_data_bucket = 'sagemaker-solutions-us-west-2'\n",
    "original_data_prefix = 'sagemaker-deep-demand-forecast/electricity'\n",
    "original_data = 's3://{}/{}'.format(original_data_bucket, original_data_prefix)\n",
    "print(\"original data: \")\n",
    "S3Downloader.list(original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will copy it to our own S3 bucket first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "bucket = 'your-s3-bucket-name'\n",
    "prefix = 'tst'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "for file in s3.list_objects(Bucket=original_data_bucket, Prefix=original_data_prefix)['Contents']:\n",
    "    copy_source = {\n",
    "      'Bucket': original_data_bucket,\n",
    "      'Key': file['Key']\n",
    "    }\n",
    "    s3.copy(copy_source, bucket, file['Key'].replace(original_data_prefix, prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = 's3://{}/{}'.format(bucket, prefix)\n",
    "print(f\"input data: {S3Downloader.list(input_data)}\")\n",
    "train_data = input_data\n",
    "preprocessed_data = 's3://{}/{}/processed_data'.format(bucket, prefix)\n",
    "train_output = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "code_location = 's3://{}/{}/code'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build container for Preprocessing and Feature Engineering\n",
    "\n",
    "Data preprocessing and feature engineering is an important component of the ML lifecycle, and Amazon SageMaker Processing allows you to do these easily on a managed infrastructure. Now, we'll create a lightweight container that will serve as the environment for our data preprocessing. The container can also be easily customized to add more dependencies when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.session.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "ecr_repository = 'sagemaker-deep-demand-forecast-preprocessing-container'\n",
    "ecr_repository_uri = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account_id,\n",
    "                                                                    region,\n",
    "                                                                    ecr_repository)\n",
    "\n",
    "!bash preprocess/container/build_and_push.sh $ecr_repository docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Preprocessing job with Amazon SageMaker Processing\n",
    "\n",
    "Since the data is already clean, the script `src/preprocess/data_preprocessor.py` demostrates schematically how to use SageMaker `ScriptProcessor` to perform some data preprocessing and feature engineering transformations on your raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                                   image_uri=ecr_repository_uri,  # we build and push above\n",
    "                                   role=role,\n",
    "                                   instance_count=1,\n",
    "                                   instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "script_processor.run(code='preprocess/data_preprocessor.py',\n",
    "                     inputs=[ProcessingInput(source=input_data,\n",
    "                                             destination='/opt/ml/processing/input')],\n",
    "                     outputs=[ProcessingOutput(destination=preprocessed_data,\n",
    "                                                source='/opt/ml/processing/output')],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results of Data Preprocessing\n",
    "\n",
    "Once the preprocessing job is complete, we can take a look at the contents of the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "processed_files = S3Downloader.list(preprocessed_data)\n",
    "print('\\n'.join(processed_files))\n",
    "\n",
    "# optionally download processed data\n",
    "# S3Downloader.download(preprocessed_data, preprocessed_data.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your LSTNet model with GluonTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTNet** is a Deep Learning model that incorporates traditional *auto-regressive* linear models *in parallel* to the non-linear neural network part, which makes the *non-linear* deep learning model more *robust* for the time series which *violate scale changes*. \n",
    "\n",
    "For more details, please checkout the paper [Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks](https://arxiv.org/abs/1703.07015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Here is a set of hyperparameters for LSTNet model for train for **1 epoch** (for demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'context_length': 12,\n",
    "    'prediction_length': 6,\n",
    "    'skip_size': 4,\n",
    "    'ar_window': 4,\n",
    "    'rnn_num_layers': 50,\n",
    "    'skip_rnn_num_layers': 50,\n",
    "    'channels': 72,\n",
    "    'epochs': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Fit SageMaker Estimator\n",
    "\n",
    "With the hyperparameters defined, we can execute the training job. We will be using the [GluonTS](https://gluon-ts.mxnet.io/), with **MXNet** as the backend deep learning framework, to define and train our *LSTNet* model. **Amazon SageMaker** makes it do this with the Framework estimators which have the deep learning frameworks already setup. Here, we create a SageMaker MXNet estimator and pass in our model training script, hyperparameters, as well as the number and type of training instances we want.\n",
    "\n",
    "We can then `fit` the estimator on the the training data location in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "estimator = MXNet(entry_point='train.py',\n",
    "                  source_dir='deep_demand_forecast',\n",
    "                  role=role,\n",
    "                  train_instance_count=1, \n",
    "                  train_instance_type='ml.p3.2xlarge', # 'ml.c4.2xlarge'\n",
    "                  framework_version=\"1.6.0\",\n",
    "                  py_version='py3',\n",
    "                  hyperparameters=hyperparameters,\n",
    "                  output_path=train_output,\n",
    "                  code_location=code_location,\n",
    "                  sagemaker_session=session,\n",
    "                  # container_log_level=10,  # debug logs\n",
    "                 )\n",
    "\n",
    "estimator.fit(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the training evaluation\n",
    "\n",
    "We can now access the training artifacts from the specified `output_path` in the above estimator and visual the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = S3Downloader.list(train_output)\n",
    "print('\\n'.join(output_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_path = os.path.join(train_output, estimator._current_job_name, 'output')\n",
    "\n",
    "S3Downloader.download(output_path, 'output')\n",
    "!tar -xvf output/output.tar.gz -C output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "item_metrics = pd.read_csv('output/item_metrics.csv.gz', compression='gzip')\n",
    "item_metrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the outputs\n",
    "\n",
    "For the visualization we will use [altair package](https://github.com/altair-viz/altair) with declarative API. If you want to export to different file formats, follow [altair_saver](https://github.com/altair-viz/altair_saver). \n",
    "\n",
    "Note that after exporting to `html` you can go to `output` and open the generated `html` files inside notebook.\n",
    "\n",
    "Here, we compare the [**Mean Absolute Scaled Error (MASE)**](https://en.wikipedia.org/wiki/Mean_absolute_scaled_error) against the [**symmetric Mean Absolute Percentage Error**](https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -q pip && pip install -q altair==4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "col_a = 'MASE'\n",
    "col_b = 'sMAPE'\n",
    "\n",
    "scatter = alt.Chart(item_metrics).mark_circle(size=100, fillOpacity=0.8).encode(\n",
    "    alt.X(col_a, scale=alt.Scale(domain=[-0.5, 9])),\n",
    "    alt.Y(col_b, scale=alt.Scale(domain=[0, 2.5])),\n",
    "    tooltip=[col_a, col_b]\n",
    ").interactive()\n",
    "scatter.save(os.path.join('output', f'{col_a}_vs_{col_b}.html'))\n",
    "scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_a_plot = alt.Chart(item_metrics).mark_bar().encode(\n",
    "        alt.X(col_a, bin=True),\n",
    "        y='count()',\n",
    ")\n",
    "col_b_plot = alt.Chart(item_metrics).mark_bar().encode(\n",
    "    alt.X(col_b, bin=True),\n",
    "    y='count()',\n",
    ")\n",
    "\n",
    "col_a_b_plot = col_a_plot | col_b_plot\n",
    "col_a_b_plot.save(os.path.join('output', f'{col_a}_{col_b}_barplots.html'))\n",
    "col_a_b_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy an endpoint\n",
    "\n",
    "To serve the model, we can deploy an endpoint where the `src/deep_demand_forecast/inference.py` script handles the predictions using the trained model as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNetModel\n",
    "\n",
    "model = MXNetModel(model_data=os.path.join(output_path, 'model.tar.gz'),\n",
    "                   role=role,\n",
    "                   entry_point='inference.py',\n",
    "                   source_dir='deep_demand_forecast',\n",
    "                   py_version='py3',\n",
    "                   framework_version='1.6.0',\n",
    "                  )\n",
    "\n",
    "predictor = model.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the endpoint\n",
    "\n",
    "Here we can test the endpoint by requesting predictions for a randomly generated data. The `predictor` handles serialization and deserialization of the requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "random_test = np.random.randn(321, 6)\n",
    "\n",
    "# json serializable request format\n",
    "test_data = {}\n",
    "test_data['target'] = random_test.tolist()\n",
    "test_data['start'] = '2014-01-01'\n",
    "test_data['source'] = []\n",
    "\n",
    "ret = predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then loads the return json objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "forecasts = np.array(ret[\"forecasts\"][\"samples\"])\n",
    "print(\"Forecasts shape with 10 samples: {}\".format(forecasts.shape))\n",
    "print(\"RMSE: {}\".format(json.loads(ret[\"agg_metrics\"])[\"RMSE\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Delete the endpoint and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're done with the endpoint, you should clean it up.\n",
    "\n",
    "All of the training jobs, models and endpoints we created can be viewed through the SageMaker console of your AWS account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
